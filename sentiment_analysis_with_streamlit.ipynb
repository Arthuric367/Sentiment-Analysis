'''
Test 1:
XGBoost hyperparameter tuning:
Increase the vectorizer max feature to 15,000 and add trigrams.

Result: GPU power is not enough to run, disconect.

Test 2:
Test 1 + enable GPU G4
/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.
  warnings.warn(
/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [04:14:19] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.

    E.g. tree_method = "hist", device = "cuda"

  bst.update(dtrain, iteration=i, fobj=obj)
Best Parameters: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01}
Best Cross-Validation Accuracy: 0.57
/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [04:14:22] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.

    E.g. tree_method = "hist", device = "cuda"

  bst.update(dtrain, iteration=i, fobj=obj)
/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [04:14:27] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.

    E.g. tree_method = "hist", device = "cuda"

  if len(data.shape) != 1 and self.num_features() != data.shape[1]:
/usr/local/lib/python3.11/dist-packages/xgboost/core.py:729: UserWarning: [04:14:27] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  return func(**kwargs)
Test Accuracy: 0.50

Test 3:
Use tree_method='hist' and device='cuda' for proper GPU support.
Convert TF-IDF output to dense format for GPU compatibility.
Reduce max_features to 10,000 and limit to bigrams (ngram_range=(1, 2)) to avoid memory issues.
Expand param_dist for better tuning.
Keep the subset approach (subset_size=10000) for tuning, then train on the full dataset.

Result:
Best Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.2, 'colsample_bytree': 0.8}
Best Cross-Validation Accuracy: 0.84
Test Accuracy: 0.87

'''

# Cell 1: Install required libraries
!pip install streamlit pyngrok pandas numpy scikit-learn nltk seaborn matplotlib xgboost==2.1.1 -q
!wget -q https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz
!tar -xvzf ngrok-v3-stable-linux-amd64.tgz -C /usr/local/bin
!chmod +x /usr/local/bin/ngrok

# Cell 2: Train XGBoost with corrected GPU setup
import pandas as pd
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix
from nltk.corpus import stopwords
import seaborn as sns
import matplotlib.pyplot as plt
import pickle
from scipy.stats import loguniform
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder

# Reinstall nltk to ensure clean setup
!pip install nltk --force-reinstall -q
nltk.download('stopwords')

# Load data
data = pd.read_csv('/content/sample_data/IMDB Dataset.csv')
reviews = data['review']
labels = data['sentiment']  # 'negative' or 'positive'

# Preprocess text
stop_words = set(stopwords.words('english'))
def preprocess(text):
    return ' '.join(word.lower() for word in text.split() if word.lower() not in stop_words)
reviews = reviews.apply(preprocess)

# Convert text to numerical features with bigrams
vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))
X = vectorizer.fit_transform(reviews)

# Encode labels ('positive'/'negative' to 1/0)
le = LabelEncoder()
y = le.fit_transform(labels)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Use subset for tuning
subset_size = 10000
X_train_subset = X_train[:subset_size].toarray()  # Convert to dense for GPU
y_train_subset = y_train[:subset_size]

# Tune XGBoost with GPU
param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}
grid = RandomizedSearchCV(XGBClassifier(tree_method='hist', device='cuda', random_state=42),
                         param_dist, n_iter=10, cv=5, n_jobs=1, random_state=42)
grid.fit(X_train_subset, y_train_subset)
print(f"Best Parameters: {grid.best_params_}")
print(f"Best Cross-Validation Accuracy: {grid.best_score_:.2f}")

# Train on full data
model = XGBClassifier(**grid.best_params_, tree_method='hist', device='cuda', random_state=42)
model.fit(X_train.toarray(), y_train)  # Convert to dense for GPU
predictions = model.predict(X_test.toarray())
print(f"Test Accuracy: {accuracy_score(y_test, predictions):.2f}")

# Show confusion matrix
cm = confusion_matrix(y_test, predictions)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Predict on new text
new_text = ["This movie was amazing!"]
new_text_transformed = vectorizer.transform(new_text).toarray()
prediction = model.predict(new_text_transformed)[0]
print(f"Prediction for sample text: {le.inverse_transform([prediction])[0]}")

# Save model, vectorizer, and label encoder
with open('model.pkl', 'wb') as f:
    pickle.dump(model, f)
with open('vectorizer.pkl', 'wb') as f:
    pickle.dump(vectorizer, f)
with open('label_encoder.pkl', 'wb') as f:
    pickle.dump(le, f)

# Cell 3: Write Streamlit app
app_code = """
import streamlit as st
import pickle
import nltk
from nltk.corpus import stopwords
import string
import numpy as np

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

with open('model.pkl', 'rb') as f:
    model = pickle.load(f)
with open('vectorizer.pkl', 'rb') as f:
    vectorizer = pickle.load(f)
with open('label_encoder.pkl', 'rb') as f:
    le = pickle.load(f)

def preprocess(text):
    text = text.lower()
    text = ''.join([char for char in text if char not in string.punctuation])
    text = ' '.join(word for word in text.split() if word not in stop_words)
    return text

st.title("Movie Review Sentiment Analysis")
st.write("Enter a movie review to predict its sentiment (positive or negative).")

user_input = st.text_area("Enter your review:", "Type your movie review here...")
if st.button("Predict Sentiment"):
    if user_input:
        processed_input = preprocess(user_input)
        input_vector = vectorizer.transform([processed_input]).toarray()
        prediction = model.predict(input_vector)[0]
        sentiment = le.inverse_transform([prediction])[0]
        st.write(f"Predicted Sentiment: **{sentiment}**")
    else:
        st.write("Please enter a review.")
"""
with open('app.py', 'w') as f:
    f.write(app_code)

# Cell 4: Run Streamlit with ngrok
from pyngrok import ngrok
import subprocess

# Set ngrok authtoken
!ngrok authtoken 2zwhy33I9ElIYpfLPxzwzMwcCZ2_6kPB1GvYwvxpWakwFZbt8
ngrok.kill()
subprocess.Popen(["streamlit", "run", "app.py", "--server.port", "8501"])
public_url = ngrok.connect(8501)
print(f"Streamlit app is live at: {public_url}")
